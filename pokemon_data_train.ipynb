{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train pokemon data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "namespace(name='pokemon_train',\n",
       "          thing_classes=['gengar'],\n",
       "          json_file='../../datasets/pokemon/v4/train/_annotations.coco.json',\n",
       "          image_root='../../datasets/pokemon/v4/train')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from detectron2.data.datasets import register_coco_instances, load_coco_json\n",
    "from detectron2.data import DatasetCatalog, MetadataCatalog\n",
    "\n",
    "DATASET_DIR=\"../../datasets/pokemon/v4/\"\n",
    "COCO_JSON_FILE_NAME=\"_annotations.coco.json\"\n",
    "\n",
    "TRAIN_JSON_PATH = DATASET_DIR + \"train/\" +COCO_JSON_FILE_NAME\n",
    "TRAIN_IMG_PATH = DATASET_DIR + \"train\"\n",
    "\n",
    "VALID_JSON_PATH = DATASET_DIR + \"valid/\" +COCO_JSON_FILE_NAME\n",
    "VALID_IMG_PATH = DATASET_DIR + \"valid\"\n",
    "\n",
    "TEST_JSON_PATH = DATASET_DIR + \"test/\" +COCO_JSON_FILE_NAME\n",
    "TEST_IMG_PATH = DATASET_DIR + \"test\"\n",
    "\n",
    "# register_coco_instances(\"pokemon_train\", {}, TRAIN_JSON_PATH, TRAIN_IMG_PATH)\n",
    "# register_coco_instances(\"pokemon_valid\", {}, VALID_JSON_PATH, VALID_IMG_PATH)\n",
    "# register_coco_instances(\"pokemon_test\", {}, TEST_JSON_PATH, TEST_IMG_PATH)\n",
    "\n",
    "DatasetCatalog.register(\"pokemon_train\", lambda: load_coco_json(TRAIN_JSON_PATH, TRAIN_IMG_PATH,\"pokemon_train\"))\n",
    "MetadataCatalog.get(\"pokemon_train\").set(thing_classes=[\"gengar\"],json_file=TRAIN_JSON_PATH,image_root=TRAIN_IMG_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random,time\n",
    "import cv2\n",
    "from detectron2.data.datasets import load_coco_json\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "from detectron2.data import MetadataCatalog\n",
    "\n",
    "dataset_train_dicts = load_coco_json(TRAIN_JSON_PATH, TRAIN_IMG_PATH, \"pokemon_train\")\n",
    "pokemon_metadata = MetadataCatalog.get(\"pokemon_train\")\n",
    "\n",
    "for d in random.sample(dataset_train_dicts, 3):\n",
    "    img = cv2.imread(d[\"file_name\"])\n",
    "    visualizer = Visualizer(img[:, :, ::-1], metadata=pokemon_metadata, scale=0.5)\n",
    "    out = visualizer.draw_dataset_dict(d)\n",
    "    cv2.imshow(str(round(time.time()*1000))+'.jpg', out.get_image()[:, :, ::-1])\n",
    "    if cv2.waitKey(0) == 27:\n",
    "        break  # esc to quit\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from detectron2 import model_zoo\n",
    "from detectron2.config import get_cfg\n",
    "\n",
    "cfg = get_cfg()\n",
    "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\"))\n",
    "cfg.DATASETS.TRAIN = (\"pokemon_train\")\n",
    "cfg.DATASETS.TEST = ()\n",
    "cfg.DATALOADER.NUM_WORKERS = 2\n",
    "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\")  # Let training initialize from model zoo\n",
    "cfg.SOLVER.IMS_PER_BATCH = 2  # This is the real \"batch size\" commonly known to deep learning people\n",
    "cfg.SOLVER.BASE_LR = 0.00025  # pick a good LR\n",
    "cfg.SOLVER.MAX_ITER = 300    # 300 iterations seems good enough for this toy dataset; you will need to train longer for a practical dataset\n",
    "cfg.SOLVER.STEPS = []        # do not decay learning rate\n",
    "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128   # The \"RoIHead batch size\". 128 is faster, and good enough for this toy dataset (default: 512)\n",
    "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 1  # only has one class (ballon). (see https://detectron2.readthedocs.io/tutorials/datasets.html#update-the-config-for-new-datasets)\n",
    "# NOTE: this config means the number of classes, but a few popular unofficial tutorials incorrect uses num_classes+1 here.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[08/08 16:55:19 d2.engine.defaults]: \u001b[0mModel:\n",
      "GeneralizedRCNN(\n",
      "  (backbone): FPN(\n",
      "    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (top_block): LastLevelMaxPool()\n",
      "    (bottom_up): ResNet(\n",
      "      (stem): BasicStem(\n",
      "        (conv1): Conv2d(\n",
      "          3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "        )\n",
      "      )\n",
      "      (res2): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (res3): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (3): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (res4): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (3): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (4): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (5): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (res5): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (proposal_generator): RPN(\n",
      "    (rpn_head): StandardRPNHead(\n",
      "      (conv): Conv2d(\n",
      "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "    (anchor_generator): DefaultAnchorGenerator(\n",
      "      (cell_anchors): BufferList()\n",
      "    )\n",
      "  )\n",
      "  (roi_heads): StandardROIHeads(\n",
      "    (box_pooler): ROIPooler(\n",
      "      (level_poolers): ModuleList(\n",
      "        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n",
      "        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n",
      "        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n",
      "        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n",
      "      )\n",
      "    )\n",
      "    (box_head): FastRCNNConvFCHead(\n",
      "      (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "      (fc1): Linear(in_features=12544, out_features=1024, bias=True)\n",
      "      (fc_relu1): ReLU()\n",
      "      (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (fc_relu2): ReLU()\n",
      "    )\n",
      "    (box_predictor): FastRCNNOutputLayers(\n",
      "      (cls_score): Linear(in_features=1024, out_features=2, bias=True)\n",
      "      (bbox_pred): Linear(in_features=1024, out_features=4, bias=True)\n",
      "    )\n",
      "    (mask_pooler): ROIPooler(\n",
      "      (level_poolers): ModuleList(\n",
      "        (0): ROIAlign(output_size=(14, 14), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n",
      "        (1): ROIAlign(output_size=(14, 14), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n",
      "        (2): ROIAlign(output_size=(14, 14), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n",
      "        (3): ROIAlign(output_size=(14, 14), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n",
      "      )\n",
      "    )\n",
      "    (mask_head): MaskRCNNConvUpsampleHead(\n",
      "      (mask_fcn1): Conv2d(\n",
      "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (mask_fcn2): Conv2d(\n",
      "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (mask_fcn3): Conv2d(\n",
      "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (mask_fcn4): Conv2d(\n",
      "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (deconv): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))\n",
      "      (deconv_relu): ReLU()\n",
      "      (predictor): Conv2d(256, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\u001b[32m[08/08 16:55:19 d2.data.datasets.coco]: \u001b[0mLoaded 37 images in COCO format from ../../datasets/pokemon/v4/train/_annotations.coco.json\n",
      "\u001b[32m[08/08 16:55:19 d2.data.build]: \u001b[0mRemoved 0 images with no usable annotations. 37 images left.\n",
      "\u001b[32m[08/08 16:55:19 d2.data.build]: \u001b[0mDistribution of instances among all 1 categories:\n",
      "\u001b[36m|  category  | #instances   |\n",
      "|:----------:|:-------------|\n",
      "|   gengar   | 43           |\n",
      "|            |              |\u001b[0m\n",
      "\u001b[32m[08/08 16:55:19 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]\n",
      "\u001b[32m[08/08 16:55:19 d2.data.build]: \u001b[0mUsing training sampler TrainingSampler\n",
      "\u001b[32m[08/08 16:55:19 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001b[32m[08/08 16:55:19 d2.data.common]: \u001b[0mSerializing 37 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[08/08 16:55:19 d2.data.common]: \u001b[0mSerialized dataset takes 0.05 MiB\n",
      "\u001b[32m[08/08 16:55:19 d2.data.build]: \u001b[0mMaking batched data loader with batch_size=2\n",
      "\u001b[32m[08/08 16:55:19 d2.checkpoint.detection_checkpoint]: \u001b[0m[DetectionCheckpointer] Loading from https://dl.fbaipublicfiles.com/detectron2/COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x/137849600/model_final_f10217.pkl ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skip loading parameter 'roi_heads.box_predictor.cls_score.weight' to the model due to incompatible shapes: (81, 1024) in the checkpoint but (2, 1024) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'roi_heads.box_predictor.cls_score.bias' to the model due to incompatible shapes: (81,) in the checkpoint but (2,) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'roi_heads.box_predictor.bbox_pred.weight' to the model due to incompatible shapes: (320, 1024) in the checkpoint but (4, 1024) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'roi_heads.box_predictor.bbox_pred.bias' to the model due to incompatible shapes: (320,) in the checkpoint but (4,) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'roi_heads.mask_head.predictor.weight' to the model due to incompatible shapes: (80, 256, 1, 1) in the checkpoint but (1, 256, 1, 1) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'roi_heads.mask_head.predictor.bias' to the model due to incompatible shapes: (80,) in the checkpoint but (1,) in the model! You might want to double check if this is expected.\n",
      "Some model parameters or buffers are not found in the checkpoint:\n",
      "\u001b[34mroi_heads.box_predictor.bbox_pred.{bias, weight}\u001b[0m\n",
      "\u001b[34mroi_heads.box_predictor.cls_score.{bias, weight}\u001b[0m\n",
      "\u001b[34mroi_heads.mask_head.predictor.{bias, weight}\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[08/08 16:55:20 d2.engine.train_loop]: \u001b[0mStarting training from iteration 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yang/miniforge3/envs/detectron2/lib/python3.8/site-packages/torch/functional.py:513: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3609.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[08/08 16:55:28 d2.utils.events]: \u001b[0m eta: 0:01:17  iter: 19  total_loss: 1.976  loss_cls: 0.7499  loss_box_reg: 0.5399  loss_mask: 0.6864  loss_rpn_cls: 0.004272  loss_rpn_loc: 0.006275    time: 0.4076  last_time: 0.6115  data_time: 0.0118  last_data_time: 0.0262   lr: 1.6068e-05  max_mem: 1583M\n",
      "\u001b[32m[08/08 16:55:36 d2.utils.events]: \u001b[0m eta: 0:01:04  iter: 39  total_loss: 1.723  loss_cls: 0.572  loss_box_reg: 0.4637  loss_mask: 0.65  loss_rpn_cls: 0.003565  loss_rpn_loc: 0.006586    time: 0.3933  last_time: 0.1719  data_time: 0.0050  last_data_time: 0.0023   lr: 3.2718e-05  max_mem: 1583M\n",
      "\u001b[32m[08/08 16:55:44 d2.utils.events]: \u001b[0m eta: 0:01:02  iter: 59  total_loss: 1.38  loss_cls: 0.3575  loss_box_reg: 0.4128  loss_mask: 0.5599  loss_rpn_cls: 0.002409  loss_rpn_loc: 0.00698    time: 0.3974  last_time: 0.2008  data_time: 0.0045  last_data_time: 0.0027   lr: 4.9367e-05  max_mem: 1583M\n",
      "\u001b[32m[08/08 16:55:51 d2.utils.events]: \u001b[0m eta: 0:00:53  iter: 79  total_loss: 1.214  loss_cls: 0.2589  loss_box_reg: 0.4988  loss_mask: 0.4458  loss_rpn_cls: 0.002171  loss_rpn_loc: 0.006311    time: 0.3915  last_time: 0.5760  data_time: 0.0031  last_data_time: 0.0023   lr: 6.6017e-05  max_mem: 1583M\n",
      "\u001b[32m[08/08 16:56:00 d2.utils.events]: \u001b[0m eta: 0:00:51  iter: 99  total_loss: 0.9671  loss_cls: 0.1821  loss_box_reg: 0.4443  loss_mask: 0.3613  loss_rpn_cls: 0.001411  loss_rpn_loc: 0.007692    time: 0.3996  last_time: 0.6950  data_time: 0.0075  last_data_time: 0.0023   lr: 8.2668e-05  max_mem: 1583M\n",
      "\u001b[32m[08/08 16:56:07 d2.utils.events]: \u001b[0m eta: 0:00:45  iter: 119  total_loss: 0.9782  loss_cls: 0.1774  loss_box_reg: 0.5123  loss_mask: 0.2751  loss_rpn_cls: 0.001866  loss_rpn_loc: 0.006843    time: 0.3950  last_time: 0.1737  data_time: 0.0077  last_data_time: 0.0029   lr: 9.9318e-05  max_mem: 1583M\n",
      "\u001b[32m[08/08 16:56:17 d2.utils.events]: \u001b[0m eta: 0:00:42  iter: 139  total_loss: 0.8173  loss_cls: 0.129  loss_box_reg: 0.4893  loss_mask: 0.2104  loss_rpn_cls: 0.001595  loss_rpn_loc: 0.007054    time: 0.4043  last_time: 0.3119  data_time: 0.0065  last_data_time: 0.0021   lr: 0.00011597  max_mem: 1583M\n",
      "\u001b[32m[08/08 16:56:25 d2.utils.events]: \u001b[0m eta: 0:00:40  iter: 159  total_loss: 0.657  loss_cls: 0.09217  loss_box_reg: 0.4005  loss_mask: 0.1494  loss_rpn_cls: 0.0006555  loss_rpn_loc: 0.007225    time: 0.4038  last_time: 0.6568  data_time: 0.0093  last_data_time: 0.0213   lr: 0.00013262  max_mem: 1583M\n",
      "\u001b[32m[08/08 16:56:33 d2.utils.events]: \u001b[0m eta: 0:00:32  iter: 179  total_loss: 0.5356  loss_cls: 0.05984  loss_box_reg: 0.3098  loss_mask: 0.1155  loss_rpn_cls: 0.0003539  loss_rpn_loc: 0.0069    time: 0.4045  last_time: 0.2367  data_time: 0.0064  last_data_time: 0.0023   lr: 0.00014927  max_mem: 1583M\n",
      "\u001b[32m[08/08 16:56:41 d2.utils.events]: \u001b[0m eta: 0:00:26  iter: 199  total_loss: 0.3995  loss_cls: 0.05353  loss_box_reg: 0.2096  loss_mask: 0.1148  loss_rpn_cls: 0.0002005  loss_rpn_loc: 0.007784    time: 0.4041  last_time: 0.3898  data_time: 0.0063  last_data_time: 0.0199   lr: 0.00016592  max_mem: 1583M\n",
      "\u001b[32m[08/08 16:56:49 d2.utils.events]: \u001b[0m eta: 0:00:21  iter: 219  total_loss: 0.3202  loss_cls: 0.04437  loss_box_reg: 0.1897  loss_mask: 0.09207  loss_rpn_cls: 0.0001788  loss_rpn_loc: 0.006355    time: 0.4044  last_time: 0.2482  data_time: 0.0036  last_data_time: 0.0022   lr: 0.00018257  max_mem: 1583M\n",
      "\u001b[32m[08/08 16:56:57 d2.utils.events]: \u001b[0m eta: 0:00:15  iter: 239  total_loss: 0.2993  loss_cls: 0.03686  loss_box_reg: 0.1631  loss_mask: 0.08182  loss_rpn_cls: 0.0001054  loss_rpn_loc: 0.004843    time: 0.4039  last_time: 0.1881  data_time: 0.0072  last_data_time: 0.0024   lr: 0.00019922  max_mem: 1583M\n",
      "\u001b[32m[08/08 16:57:05 d2.utils.events]: \u001b[0m eta: 0:00:11  iter: 259  total_loss: 0.2445  loss_cls: 0.03564  loss_box_reg: 0.1267  loss_mask: 0.07996  loss_rpn_cls: 0.0002533  loss_rpn_loc: 0.00876    time: 0.4036  last_time: 0.1789  data_time: 0.0079  last_data_time: 0.0024   lr: 0.00021587  max_mem: 1583M\n",
      "\u001b[32m[08/08 16:57:14 d2.utils.events]: \u001b[0m eta: 0:00:05  iter: 279  total_loss: 0.25  loss_cls: 0.03667  loss_box_reg: 0.1259  loss_mask: 0.07823  loss_rpn_cls: 0.0001585  loss_rpn_loc: 0.0041    time: 0.4052  last_time: 0.6333  data_time: 0.0104  last_data_time: 0.0192   lr: 0.00023252  max_mem: 1583M\n",
      "\u001b[32m[08/08 16:57:22 d2.utils.events]: \u001b[0m eta: 0:00:00  iter: 299  total_loss: 0.2163  loss_cls: 0.02835  loss_box_reg: 0.08469  loss_mask: 0.07839  loss_rpn_cls: 4.511e-05  loss_rpn_loc: 0.005198    time: 0.4052  last_time: 0.1988  data_time: 0.0049  last_data_time: 0.0023   lr: 0.00024917  max_mem: 1583M\n",
      "\u001b[32m[08/08 16:57:22 d2.engine.hooks]: \u001b[0mOverall training speed: 298 iterations in 0:02:00 (0.4052 s / it)\n",
      "\u001b[32m[08/08 16:57:22 d2.engine.hooks]: \u001b[0mTotal training time: 0:02:01 (0:00:00 on hooks)\n"
     ]
    }
   ],
   "source": [
    "from detectron2.engine import DefaultTrainer\n",
    "import os\n",
    "\n",
    "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
    "trainer = DefaultTrainer(cfg) \n",
    "trainer.resume_or_load(resume=False)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[08/08 16:57:31 d2.checkpoint.detection_checkpoint]: \u001b[0m[DetectionCheckpointer] Loading from ./output/model_final.pth ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yang/miniforge3/envs/detectron2/lib/python3.8/site-packages/fvcore/common/checkpoint.py:252: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(f, map_location=torch.device(\"cpu\"))\n"
     ]
    }
   ],
   "source": [
    "from detectron2.engine import DefaultPredictor\n",
    "import os\n",
    "\n",
    "# Inference should use the config with parameters that are used in training\n",
    "# cfg now already contains everything we've set previously. We changed it a little bit for inference:\n",
    "cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")  # path to the model we just trained\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.7   # set a custom testing threshold\n",
    "predictor = DefaultPredictor(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[08/08 16:57:47 d2.data.datasets.coco]: \u001b[0m\n",
      "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
      "\n",
      "\u001b[32m[08/08 16:57:47 d2.data.datasets.coco]: \u001b[0mLoaded 10 images in COCO format from ../../datasets/pokemon/v4/valid/_annotations.coco.json\n"
     ]
    }
   ],
   "source": [
    "from detectron2.utils.visualizer import ColorMode\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "import time, cv2\n",
    "\n",
    "dataset_dicts = load_coco_json(DATASET_DIR + \"valid/\" +COCO_JSON_FILE_NAME, DATASET_DIR + \"valid\", \"pokemon_train_dataset\")\n",
    "\n",
    "for d in random.sample(dataset_dicts, 3 if len(dataset_dicts)>3 else len(dataset_dicts)):    \n",
    "    # im = cv2.imread(d[\"file_name\"])\n",
    "    im = cv2.imread(\"../../datasets/pokemon/\" + \"2.jpeg\")\n",
    "    outputs = predictor(im)  # format is documented at https://detectron2.readthedocs.io/tutorials/models.html#model-output-format\n",
    "    v = Visualizer(im[:, :, ::-1],\n",
    "                   metadata=pokemon_metadata, \n",
    "                   scale=0.5, \n",
    "                   instance_mode=ColorMode.IMAGE_BW   # remove the colors of unsegmented pixels. This option is only available for segmentation models\n",
    "    )\n",
    "    out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
    "    image_name = str(round(time.time()*1000))+'.jpg'\n",
    "    cv2.imshow(image_name, out.get_image()[:, :, ::-1])\n",
    "    if cv2.waitKey(0) == 27:\n",
    "        break  # esc to quit\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[08/08 16:10:10 d2.evaluation.coco_evaluation]: \u001b[0mTrying to convert 'pokemon_valid' to COCO format ...\n",
      "\u001b[32m[08/08 16:10:10 d2.data.datasets.coco]: \u001b[0mConverting annotations of dataset 'pokemon_valid' to COCO format ...)\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"Dataset 'pokemon_valid' is not registered! Available datasets are: coco_2014_train, coco_2014_val, coco_2014_minival, coco_2014_valminusminival, coco_2017_train, coco_2017_val, coco_2017_test, coco_2017_test-dev, coco_2017_val_100, keypoints_coco_2014_train, keypoints_coco_2014_val, keypoints_coco_2014_minival, keypoints_coco_2014_valminusminival, keypoints_coco_2017_train, keypoints_coco_2017_val, keypoints_coco_2017_val_100, coco_2017_train_panoptic_separated, coco_2017_train_panoptic_stuffonly, coco_2017_train_panoptic, coco_2017_val_panoptic_separated, coco_2017_val_panoptic_stuffonly, coco_2017_val_panoptic, coco_2017_val_100_panoptic_separated, coco_2017_val_100_panoptic_stuffonly, coco_2017_val_100_panoptic, lvis_v1_train, lvis_v1_val, lvis_v1_test_dev, lvis_v1_test_challenge, lvis_v0.5_train, lvis_v0.5_val, lvis_v0.5_val_rand_100, lvis_v0.5_test, lvis_v0.5_train_cocofied, lvis_v0.5_val_cocofied, cityscapes_fine_instance_seg_train, cityscapes_fine_sem_seg_train, cityscapes_fine_instance_seg_val, cityscapes_fine_sem_seg_val, cityscapes_fine_instance_seg_test, cityscapes_fine_sem_seg_test, cityscapes_fine_panoptic_train, cityscapes_fine_panoptic_val, voc_2007_trainval, voc_2007_train, voc_2007_val, voc_2007_test, voc_2012_trainval, voc_2012_train, voc_2012_val, ade20k_sem_seg_train, ade20k_sem_seg_val, pokemon_train\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/Documents/Playground/detectron2/detectron2/data/catalog.py:51\u001b[0m, in \u001b[0;36m_DatasetCatalog.get\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m     f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/miniforge3/envs/detectron2/lib/python3.8/collections/__init__.py:1010\u001b[0m, in \u001b[0;36mUserDict.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1009\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__missing__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key)\n\u001b[0;32m-> 1010\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'pokemon_valid'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdetectron2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mevaluation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m COCOEvaluator, inference_on_dataset\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdetectron2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m build_detection_test_loader\n\u001b[0;32m----> 3\u001b[0m evaluator \u001b[38;5;241m=\u001b[39m \u001b[43mCOCOEvaluator\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpokemon_valid\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./output\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m val_loader \u001b[38;5;241m=\u001b[39m build_detection_test_loader(cfg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpokemon_valid\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(inference_on_dataset(predictor\u001b[38;5;241m.\u001b[39mmodel, val_loader, evaluator))\n",
      "File \u001b[0;32m~/Documents/Playground/detectron2/detectron2/evaluation/coco_evaluation.py:142\u001b[0m, in \u001b[0;36mCOCOEvaluator.__init__\u001b[0;34m(self, dataset_name, tasks, distributed, output_dir, max_dets_per_image, use_fast_impl, kpt_oks_sigmas, allow_cached_coco)\u001b[0m\n\u001b[1;32m    140\u001b[0m     cache_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(output_dir, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_coco_format.json\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metadata\u001b[38;5;241m.\u001b[39mjson_file \u001b[38;5;241m=\u001b[39m cache_path\n\u001b[0;32m--> 142\u001b[0m     \u001b[43mconvert_to_coco_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_cached\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_cached_coco\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    144\u001b[0m json_file \u001b[38;5;241m=\u001b[39m PathManager\u001b[38;5;241m.\u001b[39mget_local_path(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metadata\u001b[38;5;241m.\u001b[39mjson_file)\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mredirect_stdout(io\u001b[38;5;241m.\u001b[39mStringIO()):\n",
      "File \u001b[0;32m~/Documents/Playground/detectron2/detectron2/data/datasets/coco.py:480\u001b[0m, in \u001b[0;36mconvert_to_coco_json\u001b[0;34m(dataset_name, output_file, allow_cached)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    479\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConverting annotations of dataset \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m to COCO format ...)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 480\u001b[0m     coco_dict \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_to_coco_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    482\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCaching COCO format annotations at \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m ...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    483\u001b[0m     tmp_file \u001b[38;5;241m=\u001b[39m output_file \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.tmp\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/Documents/Playground/detectron2/detectron2/data/datasets/coco.py:331\u001b[0m, in \u001b[0;36mconvert_to_coco_dict\u001b[0;34m(dataset_name)\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconvert_to_coco_dict\u001b[39m(dataset_name):\n\u001b[1;32m    312\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    313\u001b[0m \u001b[38;5;124;03m    Convert an instance detection/segmentation or keypoint detection dataset\u001b[39;00m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;124;03m    in detectron2's standard format into COCO json format.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    328\u001b[0m \u001b[38;5;124;03m        coco_dict: serializable dict in COCO json format\u001b[39;00m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 331\u001b[0m     dataset_dicts \u001b[38;5;241m=\u001b[39m \u001b[43mDatasetCatalog\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    332\u001b[0m     metadata \u001b[38;5;241m=\u001b[39m MetadataCatalog\u001b[38;5;241m.\u001b[39mget(dataset_name)\n\u001b[1;32m    334\u001b[0m     \u001b[38;5;66;03m# unmap the category mapping ids for COCO\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Playground/detectron2/detectron2/data/catalog.py:53\u001b[0m, in \u001b[0;36m_DatasetCatalog.get\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m     f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m[name]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m---> 53\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\n\u001b[1;32m     54\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is not registered! Available datasets are: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m     55\u001b[0m             name, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeys()))\n\u001b[1;32m     56\u001b[0m         )\n\u001b[1;32m     57\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m f()\n",
      "\u001b[0;31mKeyError\u001b[0m: \"Dataset 'pokemon_valid' is not registered! Available datasets are: coco_2014_train, coco_2014_val, coco_2014_minival, coco_2014_valminusminival, coco_2017_train, coco_2017_val, coco_2017_test, coco_2017_test-dev, coco_2017_val_100, keypoints_coco_2014_train, keypoints_coco_2014_val, keypoints_coco_2014_minival, keypoints_coco_2014_valminusminival, keypoints_coco_2017_train, keypoints_coco_2017_val, keypoints_coco_2017_val_100, coco_2017_train_panoptic_separated, coco_2017_train_panoptic_stuffonly, coco_2017_train_panoptic, coco_2017_val_panoptic_separated, coco_2017_val_panoptic_stuffonly, coco_2017_val_panoptic, coco_2017_val_100_panoptic_separated, coco_2017_val_100_panoptic_stuffonly, coco_2017_val_100_panoptic, lvis_v1_train, lvis_v1_val, lvis_v1_test_dev, lvis_v1_test_challenge, lvis_v0.5_train, lvis_v0.5_val, lvis_v0.5_val_rand_100, lvis_v0.5_test, lvis_v0.5_train_cocofied, lvis_v0.5_val_cocofied, cityscapes_fine_instance_seg_train, cityscapes_fine_sem_seg_train, cityscapes_fine_instance_seg_val, cityscapes_fine_sem_seg_val, cityscapes_fine_instance_seg_test, cityscapes_fine_sem_seg_test, cityscapes_fine_panoptic_train, cityscapes_fine_panoptic_val, voc_2007_trainval, voc_2007_train, voc_2007_val, voc_2007_test, voc_2012_trainval, voc_2012_train, voc_2012_val, ade20k_sem_seg_train, ade20k_sem_seg_val, pokemon_train\""
     ]
    }
   ],
   "source": [
    "from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n",
    "from detectron2.data import build_detection_test_loader\n",
    "evaluator = COCOEvaluator(\"pokemon_valid\", output_dir=\"./output\")\n",
    "val_loader = build_detection_test_loader(cfg, \"pokemon_valid\")\n",
    "print(inference_on_dataset(predictor.model, val_loader, evaluator))\n",
    "# another equivalent way to evaluate the model is to use `trainer.test`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "detectron2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
